{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **[Week 10] Regularization**\n",
        "\n",
        "* Motivation\n",
        "* Implement regularization: $l_1$, $l_2$, dropout.\n",
        "\n",
        "**[Important]** The results of the exercises should be included in your weekly report for this practice session. The weekly report for this session should be submitted to KLMS by this weekend. No late submission would be accepted."
      ],
      "metadata": {
        "id": "XK7jbghmyVGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Motivation  \n",
        "Regularizatoin technique is a strategy used to prevent overfitting, enhance model generalization, and improve its performance on unseen data.\n",
        "In this part, we understand why we use regularization technique.  \n",
        "We observe overfitting when we use a model with high complexity and observe mitigation of overffing when we use regularization."
      ],
      "metadata": {
        "id": "-uqUjokKewnr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQNwl7F3M1B2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Set random seeds fixed for reproducibility\n",
        "seed = 7    # Do NOT change the seed\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate synthetic data"
      ],
      "metadata": {
        "id": "7V7HOKwpr42D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# synthetic data\n",
        "# ground truth: y=0.05 for all x\n",
        "X = torch.tensor([-1,-0.5,0,0.5,1])\n",
        "y = torch.tensor([0.05,0.05,0.05,0.05,0.05])\n",
        "noise = torch.randn(5)*0.05\n",
        "y=y+noise\n",
        "plt.plot(X,y,'o')\n",
        "\n",
        "X_grid = torch.linspace(-1,1,201)\n",
        "plt.plot(X_grid,[0.05]*len(X_grid),label='ground truth')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1t0VKtIw8NQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define fourth-order model"
      ],
      "metadata": {
        "id": "1Q_AutLDsFUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# high order polynomial model\n",
        "class PolyRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PolyRegression,self).__init__()\n",
        "    self.theta = nn.Parameter(torch.zeros(5))\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.theta[0] + self.theta[1] * x + self.theta[2] * x**2 + self.theta[3] * x**3 + self.theta[4] * x**4\n",
        "\n",
        "poly_model = PolyRegression()\n",
        "poly_model.theta"
      ],
      "metadata": {
        "id": "06Dg5RgVeijd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the models"
      ],
      "metadata": {
        "id": "mHAs9Ea8sIRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop\n",
        "n_epochs = 2000\n",
        "\n",
        "#optimizer\n",
        "optP = optim.SGD(poly_model.parameters(),lr=0.3)\n",
        "\n",
        "loss_list_P = list()\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  predP = poly_model(X) #y_hat\n",
        "\n",
        "  lossP = torch.sum((predP - y)**2)/len(X)\n",
        "  loss_list_P.append(lossP.item())\n",
        "\n",
        "  optP.zero_grad()\n",
        "  lossP.backward()\n",
        "  optP.step()"
      ],
      "metadata": {
        "id": "4hF6_STjeihD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"coefficients of polynomial model: \",poly_model.theta)"
      ],
      "metadata": {
        "id": "MXpZMvtAeie4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_list_P,'r',label='fourth-order')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lNEeHxfzeicQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# five points\n",
        "plt.plot(X,y,'o')\n",
        "\n",
        "X_grid = torch.linspace(-1,1,201)\n",
        "\n",
        "predP = [yhat.detach() for yhat in poly_model(X_grid)]\n",
        "plt.plot(X_grid,predP,'k',label='fourth-order')\n",
        "\n",
        "plt.plot(X_grid,[0.05]*len(X_grid),label='ground truth')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pTlwXL4y2RZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, add $l_2$ regression  \n",
        "Here, we don't explicitly design $l_2$ regularization but use argument \"weight_decay\" of optim.SGD.  \n",
        "You will design $l_2$ regularization later."
      ],
      "metadata": {
        "id": "LiNo69FSrv9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_model_reg = PolyRegression()"
      ],
      "metadata": {
        "id": "2As1ZtuaeiUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop\n",
        "n_epochs = 2000\n",
        "\n",
        "#optimizer\n",
        "optP_reg = optim.SGD(poly_model_reg.parameters(),lr=0.005,weight_decay=1)\n",
        "\n",
        "loss_list_P_reg = list()\n",
        "\n",
        "for e in range(n_epochs):\n",
        "  predP_reg = poly_model_reg(X) #y_hat\n",
        "\n",
        "  lossP_reg = torch.sum((predP_reg - y)**2)/len(X)\n",
        "  loss_list_P_reg.append(lossP_reg.item())\n",
        "\n",
        "  optP_reg.zero_grad()\n",
        "  lossP_reg.backward()\n",
        "  optP_reg.step()"
      ],
      "metadata": {
        "id": "IZWi1v_peiSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly_model_reg.theta"
      ],
      "metadata": {
        "id": "geIZxNnHeiPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the training loss curves for cases with and without regularization."
      ],
      "metadata": {
        "id": "UpLys4YMV1Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train loss\n",
        "plt.plot(loss_list_P,'r',label='no reg')\n",
        "plt.plot(loss_list_P_reg,'m',label='reg')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hi1emCzrsRWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X,y,'o')\n",
        "\n",
        "X_grid = torch.linspace(-1,1,201)\n",
        "\n",
        "predP = [yhat.detach() for yhat in poly_model(X_grid)]\n",
        "plt.plot(X_grid,predP,'k',label='no reg')\n",
        "predP_reg = [yhat.detach() for yhat in poly_model_reg(X_grid)]\n",
        "plt.plot(X_grid,predP_reg,'m',label='reg')\n",
        "\n",
        "plt.plot(X_grid,[0.05]*len(X_grid),label='ground truth')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sgLITHm_eiM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remark  \n",
        "Without regularization, model with high complexity has overfitted to the given dataset(black graph).   \n",
        "By adding regularization, overfitting has mitigated(purple graph).  \n",
        "This simple example illustrates the role that regularization techniques play."
      ],
      "metadata": {
        "id": "ocztkQdxa4Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Implement regularization: $l_1,l_2$, dropout.  \n",
        "We implement several regularization techniques with binary classification task.  \n",
        "We practiced this task in the week 06 for regression."
      ],
      "metadata": {
        "id": "ISab5Q6HNtLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-1. Recall binary classification with Titanic dataset"
      ],
      "metadata": {
        "id": "fwe4NPC6KXUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required packages"
      ],
      "metadata": {
        "id": "a-mgZsfXEdVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Y2UUTz1XI7Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and preprocess the Titanic dataset"
      ],
      "metadata": {
        "id": "Nzg1-kiwEmAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL to the Titanic dataset\n",
        "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
        "\n",
        "# Make a GET request to the URL and read the content into a pandas DataFrame\n",
        "response = requests.get(url).text\n",
        "data = pd.read_csv(StringIO(response))"
      ],
      "metadata": {
        "id": "YLVKWARNI7PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "data.dropna(subset=['Age', 'Fare'], inplace=True) # empty -> drop\n",
        "X = data[['Pclass', 'Sex', 'Age', 'Fare']]\n",
        "y = data['Survived']\n",
        "X = pd.get_dummies(X, columns=['Pclass', 'Sex'], drop_first=True) # one-hot encoding\n",
        "# drop the first to reduce complexity e.g. Pclass=3->001->00, Pclass=1->100->00, male->1, female->0"
      ],
      "metadata": {
        "id": "c1ev-0WlI7Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To observe the effect of regularization, we try an extreme case where the size of training dataset is small."
      ],
      "metadata": {
        "id": "CsuAuUn_Rp1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.95, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "qlD3tkSJI7JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the size of training dataset\n",
        "len(X_train)"
      ],
      "metadata": {
        "id": "k-F9_8oSLTmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32) #, requires_grad=True)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32) #, requires_grad=True)\n",
        "y_train_tensor = y_train.astype(float)\n",
        "y_test_tensor = y_test.astype(float)\n",
        "y_train_tensor = torch.tensor(y_train_tensor.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test_tensor.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "bgpFX-3ZI7Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define classificaiton model"
      ],
      "metadata": {
        "id": "ICH6WwSyEv89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1.**  \n",
        "Define functions **compute_l1_norm** and **compute_l2_norm**.  \n",
        "* input: flattened vector\n",
        "* output: $l_1$-norm and $l_2$-norm, respectively."
      ],
      "metadata": {
        "id": "YfHIEHgQHIQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NN,self).__init__()\n",
        "    self.fc1 = nn.Linear(5,16)\n",
        "    self.fc2 = nn.Linear(16,8)\n",
        "    self.fc3 = nn.Linear(8,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    x = nn.ReLU()(x)\n",
        "    x = self.fc2(x)\n",
        "    x = nn.ReLU()(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "  def compute_l1_norm(self, w):\n",
        "    ####### Task 1 #######\n",
        "\n",
        "    ######################\n",
        "\n",
        "  def compute_l2_norm(self, w):\n",
        "    ####### Task 1 #######\n",
        "\n",
        "    ######################"
      ],
      "metadata": {
        "id": "2KBnVrnyI7Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-2. Classification with no regularization"
      ],
      "metadata": {
        "id": "CoOOm67oE96x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model, loss and optimizer.  \n",
        "Run the main loop without regularization."
      ],
      "metadata": {
        "id": "onfI_yCIGpAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No regularization\n",
        "\n",
        "regmodel=NN() #regression model\n",
        "regmodel.to(torch.float32)\n",
        "\n",
        "# Define your loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "#optimizer = optim.SGD(regmodel.parameters(), lr=0.01)\n",
        "optimizer = optim.Adam(regmodel.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 3000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "    out = regmodel(X_train_tensor)                     # Forward pass\n",
        "    p=torch.cat((torch.zeros_like(out),out),dim=1)     # binary probability distribution\n",
        "    loss = criterion(p, y_train_tensor)                # Compute the loss\n",
        "\n",
        "    loss.backward()        # Backpropagate to compute gradients\n",
        "    optimizer.step()       # Update the model's parameters"
      ],
      "metadata": {
        "id": "BmWTLBqBI68T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your model"
      ],
      "metadata": {
        "id": "1yMdnHYhHVOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regmodel.eval()\n",
        "with torch.no_grad():\n",
        "  # Predictions\n",
        "  y_pred = regmodel(X_test_tensor)\n",
        "  y_pred = [y.detach().numpy()>=0 for y in y_pred]\n",
        "\n",
        "  # Convert true labels tensor to numpy array\n",
        "  y_test_np = y_test_tensor.numpy()\n",
        "\n",
        "  # Evaluate the model\n",
        "  accuracy = accuracy_score(y_test_np, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "XNSw7ao7I64Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-3. $l_1$ regularization    \n",
        "Let the loss function without regularization be denoted as $L(\\theta)$.  \n",
        "In this part, you will implement $l_1$ regularization: $L_{1}(\\theta)=L(\\theta)+\\lambda\\Sigma_i|\\theta_i|$"
      ],
      "metadata": {
        "id": "keBsq04zFJPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model, loss and optimizer.  \n",
        "Run the main loop with $l_1$ regularization."
      ],
      "metadata": {
        "id": "Q-wqqfv3GuE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2.**  \n",
        "Define the loss function with $l_1$-regularization.  \n",
        "Use **compute_l1_norm** method."
      ],
      "metadata": {
        "id": "5i_hn2PgJr9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# l1 regularization\n",
        "l1_lambda = 5e-4\n",
        "\n",
        "regmodel_l1=NN() #regression model\n",
        "regmodel_l1.to(torch.float32)\n",
        "\n",
        "# Define your loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "#optimizer = optim.SGD(regmodel_l1.parameters(), lr=0.01)\n",
        "optimizer = optim.Adam(regmodel_l1.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 3000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "    out = regmodel_l1(X_train_tensor)                     # Forward pass\n",
        "    p=torch.cat((torch.zeros_like(out),out),dim=1)     # binary probability distribution\n",
        "    loss = criterion(p, y_train_tensor)                # Compute the loss\n",
        "\n",
        "    params = []\n",
        "    for param in regmodel_l1.parameters():\n",
        "        params.append(param.view(-1))\n",
        "\n",
        "    ####### Task 2 ######\n",
        "    ### Add proper regularization term using the method \"compute_l1_norm\" ###\n",
        "\n",
        "\n",
        "    #####################\n",
        "\n",
        "    loss.backward()        # Backpropagate to compute gradients\n",
        "    optimizer.step()       # Update the model's parameters"
      ],
      "metadata": {
        "id": "Z_yVLb0ZI616"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your model"
      ],
      "metadata": {
        "id": "RLdKt1t9TlaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regmodel_l1.eval()\n",
        "with torch.no_grad():\n",
        "  # Predictions\n",
        "  y_pred = regmodel_l1(X_test_tensor)\n",
        "  y_pred = [y.detach().numpy()>=0 for y in y_pred]\n",
        "\n",
        "  # Convert true labels tensor to numpy array\n",
        "  y_test_np = y_test_tensor.numpy()\n",
        "\n",
        "  # Evaluate the model\n",
        "  accuracy = accuracy_score(y_test_np, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "5H9Dh4LNJpzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-4. $l_2$ regularization  \n",
        "In this part, you will implement $l_2$ regularization: $L_{2}(\\theta)=L(\\theta)+\\frac{\\lambda}{2}\\Sigma_i\\theta_i^2$"
      ],
      "metadata": {
        "id": "2gG7D2KcGZbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model, loss and optimizer.  \n",
        "Run the main loop with $l_2$ regularization."
      ],
      "metadata": {
        "id": "CN1MXxwXG2vT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3.**   \n",
        "Define the loss function with $l_2$-regularization.  \n",
        "Use **compute_l2_norm** method."
      ],
      "metadata": {
        "id": "5unYistqKaHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# l2 regularization\n",
        "l2_lambda = 5e-4\n",
        "\n",
        "regmodel_l2=NN() #regression model\n",
        "regmodel_l2.to(torch.float32)\n",
        "\n",
        "# Define your loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "#optimizer = optim.SGD(regmodel_l2.parameters(), lr=0.01)\n",
        "optimizer = optim.Adam(regmodel_l2.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 3000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "    out = regmodel_l2(X_train_tensor)                     # Forward pass\n",
        "    p=torch.cat((torch.zeros_like(out),out),dim=1)     # binary probability distribution\n",
        "    loss = criterion(p, y_train_tensor)                # Compute the loss\n",
        "\n",
        "    params = []\n",
        "    for param in regmodel_l2.parameters():\n",
        "        params.append(param.view(-1))\n",
        "\n",
        "    ####### Task 3 ######\n",
        "    ### Add proper regularization term using the method \"compute_l2_norm\" ###\n",
        "\n",
        "\n",
        "    #####################\n",
        "\n",
        "    loss.backward()        # Backpropagate to compute gradients\n",
        "    optimizer.step()       # Update the model's parameters"
      ],
      "metadata": {
        "id": "hKmVtqyDJpxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your model"
      ],
      "metadata": {
        "id": "5ZlP8duBT7oa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regmodel_l2.eval()\n",
        "with torch.no_grad():\n",
        "  # Predictions\n",
        "  y_pred = regmodel_l2(X_test_tensor)\n",
        "  y_pred = [y.detach().numpy()>=0 for y in y_pred]\n",
        "\n",
        "  # Convert true labels tensor to numpy array\n",
        "  y_test_np = y_test_tensor.numpy()\n",
        "\n",
        "  # Evaluate the model\n",
        "  accuracy = accuracy_score(y_test_np, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "RCqUnE1BJptp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remark for $l_2$ regularization  \n",
        "Recall the loss $L_{2}(\\theta)=L(\\theta)+\\frac{\\lambda}{2}\\Sigma_i\\theta_i^2$. By taking derivative, obtain  \n",
        "\\begin{align}\n",
        "    \\nabla_\\theta L_2(\\theta) = \\nabla_\\theta L(\\theta) + \\lambda\\theta\n",
        "\\end{align}\n",
        "Applying gradient descent step for $L_2$, update rule becomes\n",
        "\\begin{align}\n",
        "    \\theta \\leftarrow &\\theta-\\alpha(\\nabla_\\theta L(\\theta) + \\lambda\\theta)\\\\\n",
        "    =&(1-\\alpha\\lambda)\\theta-\\alpha\\nabla_\\theta L(\\theta)\n",
        "\\end{align}\n",
        "which can be interpreted as gradient descent for loss $L$ with decaying the current $\\theta$.  \n",
        "This is why the hyperparameter $\\lambda$ is usually called \"weight decay\". (weight $\\theta$ is decayed by factor $1-\\alpha\\lambda$)"
      ],
      "metadata": {
        "id": "8buo9eiMUEtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2-5. Dropout"
      ],
      "metadata": {
        "id": "hqqmMi-MT-kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Dropout is a regularization technique for neural networks to mitigate overfitting.   \n",
        "* Dropout rate $p\\in[0,1]$ indicates the proportion of neurons to be dropped out.  \n",
        " In other words, each neuron in dropout layer with rate $p$ is dropped out with probability $p$.\n",
        "* Dropout prevents any single neuron from relying too heavily on specific input features. Therefore, the model can learn more diverse and generalized representations.  \n",
        "* When implementing dropout, the size of input is not changed but some neurons are set to 0.  \n",
        "\n",
        "\n",
        "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/C9sJbyk/dropout.png\" alt=\"dropout\" border=\"0\"></a>  \n",
        "image source - Dropout: A Simple Way to Prevent Neural Networks from\n",
        "Overfitting"
      ],
      "metadata": {
        "id": "PuyilVeYXDMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4.**   \n",
        "Design your own network with dropout.  \n",
        "Dropout rate does not have to be 0.5.  \n",
        "You can add dropout layers wherever you want.   \n",
        "The only requirement is that you must use dropout at least once."
      ],
      "metadata": {
        "id": "n1isMbtmYwwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_Dropout(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NN_Dropout,self).__init__()\n",
        "    self.fc1 = nn.Linear(5,16)\n",
        "    self.fc2 = nn.Linear(16,8)\n",
        "    self.fc3 = nn.Linear(8,1)\n",
        "    self.dropout = nn.Dropout(0.5) # You can change the dropout rate\n",
        "\n",
        "  def forward(self,x):\n",
        "    ###### Task 4 #######\n",
        "\n",
        "\n",
        "    #####################\n",
        "    return x"
      ],
      "metadata": {
        "id": "Hve2yReaJprc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model, loss and optimizer.  \n",
        "Run the main loop with dropout."
      ],
      "metadata": {
        "id": "bvU6fc2rU9XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout regularization\n",
        "\n",
        "regmodel_dropout=NN_Dropout() #regression model\n",
        "regmodel_dropout.to(torch.float32)\n",
        "\n",
        "# Define your loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "#optimizer = optim.SGD(regmodel_dropout.parameters(), lr=0.01)\n",
        "optimizer = optim.Adam(regmodel_dropout.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 3000\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "    out = regmodel_dropout(X_train_tensor)                     # Forward pass\n",
        "    p=torch.cat((torch.zeros_like(out),out),dim=1)     # binary probability distribution\n",
        "    loss = criterion(p, y_train_tensor)                # Compute the loss\n",
        "\n",
        "    loss.backward()        # Backpropagate to compute gradients\n",
        "    optimizer.step()       # Update the model's parameters"
      ],
      "metadata": {
        "id": "xPOjIbXhJpph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your model"
      ],
      "metadata": {
        "id": "ccvxRZaOIUoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regmodel_dropout.eval()\n",
        "with torch.no_grad():\n",
        "  # Predictions\n",
        "  y_pred = regmodel_dropout(X_test_tensor)\n",
        "  y_pred = [y.detach().numpy()>=0 for y in y_pred]\n",
        "\n",
        "  # Convert true labels tensor to numpy array\n",
        "  y_test_np = y_test_tensor.numpy()\n",
        "\n",
        "  # Evaluate the model\n",
        "  accuracy = accuracy_score(y_test_np, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "yJDeSBgwJpnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reference  \n",
        "https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-l1-l2-and-elastic-net-regularization-with-pytorch.md"
      ],
      "metadata": {
        "id": "8nwszcuokbKx"
      }
    }
  ]
}