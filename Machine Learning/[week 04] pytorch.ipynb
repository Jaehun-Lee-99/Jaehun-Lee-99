{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"colab":{"provenance":[],"collapsed_sections":["8vUJG0Ofsp9L","S5eWKE4Nsp9R"],"toc_visible":true,"gpuType":"T4"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QbjN-8O3sp9I"},"source":["# [Week 4] PyTorch\n","\n","In this lab session, we are going to cover the basics of PyTorch Framework, next let's implement a linear model and train it with PyTorch"]},{"cell_type":"markdown","metadata":{"id":"0_6tgh_lsp9I"},"source":["## Why PyTorch?"]},{"cell_type":"markdown","metadata":{"id":"EMwIjuVvsp9J"},"source":["- A popular deep learning framework\n","- Ease of use, flexibility, efficient memory usage, intuitive and concise code\n","- Python-like coding, high compatibility with Numpy"]},{"cell_type":"markdown","metadata":{"id":"6O3b-lLfsp9J"},"source":["![picture](https://www.assemblyai.com/blog/content/images/2023/01/percentage_repo_2023.png)"]},{"cell_type":"markdown","metadata":{"id":"8vUJG0Ofsp9L"},"source":["## PyTorch and Numpy"]},{"cell_type":"markdown","metadata":{"id":"qG5rTO8tsp9L"},"source":["PyTorch provides a data structure called **tensor**, which is almost the same with NumPy\\\n","(Also it supports GPU acceleration, look into this later)"]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","torch.__version__"],"metadata":{"id":"H3EmwFj0lAMJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N5RrHkDdsp9O"},"source":["### PyTorch tensor and NumPy array"]},{"cell_type":"code","metadata":{"id":"dT6LsNonsp9O"},"source":["np_array = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n","print(np_array)\n","\n","torch_tensor = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n","print(torch_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["numpy_to_torch = torch.tensor(np_array)\n","print(numpy_to_torch)\n","\n","torch_to_numpy = numpy_to_torch.numpy()\n","print(torch_to_numpy)"],"metadata":{"id":"oOW52cFExvMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Same operations with same grammer"],"metadata":{"id":"WmmoqHRCl69a"}},{"cell_type":"code","metadata":{"id":"tVKGdRwNsp9P"},"source":["# numpy\n","print(np_array.shape)\n","\n","# torch\n","print(torch_tensor.shape)\n","print(torch_tensor.size())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# numpy\n","np_array_ones = np.ones_like(np_array)\n","print(np_array_ones)\n","print(np_array_ones.shape)\n","\n","# torch\n","torch_tensor_ones = torch.ones_like(torch_tensor)\n","print(torch_tensor_ones)\n","print(torch_tensor_ones.size())"],"metadata":{"id":"eoTAqt-1mEod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# numpy\n","np_array_1, np_array_2 = np.array([1, 2]), np.array([3, 4])\n","print(np_array_1 + np_array_2)\n","\n","# torch\n","torch_tensor_1, torch_tensor_2 = torch.tensor([1, 2]), torch.tensor([3, 4])\n","print(torch_tensor_1 + torch_tensor_2)"],"metadata":{"id":"QKuA6Ds1rvei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# numpy\n","print(np_array_1@np_array_2 , np.matmul(np_array_1, np_array_2))\n","\n","# torch\n","print(torch_tensor_1@torch_tensor_2, torch.matmul(torch_tensor_1, torch_tensor_2))"],"metadata":{"id":"iXwSZSJ7owct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Ph6yZ4Nsp9P"},"source":["### Same operations with different grammer"]},{"cell_type":"code","metadata":{"id":"t6AZYswBsp9P"},"source":["# numpy\n","np_concat = np.concatenate([np_array, np_array_ones], axis=0)\n","print(np_concat)\n","print(np_concat.shape)\n","\n","# torch\n","torch_concat= torch.cat([torch_tensor, torch_tensor_ones], dim=0)\n","print(torch_concat)\n","print(torch_concat.size())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBMJhl6tsp9Q"},"source":["# numpy\n","np_reshaped = np_concat.reshape(4, -1)\n","print (np_reshaped)\n","print (np_reshaped.shape)\n","\n","# torch\n","torch_reshaped = torch_concat.view(4, -1)\n","print (torch_reshaped)\n","print (torch_reshaped.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# numpy\n","np_clone = np_reshaped.copy()\n","print(np_clone)\n","\n","# torch\n","torch_clone = torch_reshaped.clone()\n","print(torch_clone)"],"metadata":{"id":"JvbKUw06ngaR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### PyTorch for NumPy users\n","For more examples, see [here](https://github.com/wkentaro/pytorch-for-numpy-users)."],"metadata":{"id":"1LX417TeIsc_"}},{"cell_type":"markdown","metadata":{"id":"S5eWKE4Nsp9R"},"source":["## Tensor operations under GPU utilization"]},{"cell_type":"markdown","metadata":{"id":"pXgOePsysp9R"},"source":["Deep learning framework driving the successful machine learning utilizes high-performance GPU to accelarate its computation.\n","\n","Let's learn how to utilize GPU in PyTorch\n","\n","Check the hardware accelerator for your Colab!\\\n","**Runtime -> Change runtime type -> T4 GPU (Hardware accelerator)**"]},{"cell_type":"code","source":["# current GPU usages in your Colab\n","!nvidia-smi"],"metadata":{"id":"OUOrp51tus4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEnKzXeIsp9S"},"source":["# could you access GPU?\n","print(torch.cuda.is_available())"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","device"],"metadata":{"id":"TvAxLeQTzNVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4QF2qY3sp9S"},"source":["# on CPU\n","a = torch.ones(3)\n","b = torch.rand(3)\n","\n","print(a, a.device)\n","print(b, b.device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sa9qkmausp9S"},"source":["c = a + b\n","\n","print(c, c.device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBmLLHZ5sp9U"},"source":["# to GPU\n","a = a.to('cuda')\n","b = b.to('cuda')\n","\n","print(a, a.device)\n","print(b, b.device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhoePABXsp9U"},"source":["c = a + b\n","\n","print(c, c.device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7cu4nPxrsp9V"},"source":["# back to CPU\n","c = c.to('cpu')\n","\n","print(c, c.device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xd96DeWesp9V"},"source":["## Autograd (Automatic Differentiation)\n","\n","Most machine learning (or deep learning) frameworks train your model by optimizing it through gradient descent (a first-order iterative optimization algorithm).\\\n","For this, PyTorch provides ```torch.autograd``` to automatic differentiate for all operations on PyTorch tensors.\n","\n","Let's see this with some examples."]},{"cell_type":"markdown","metadata":{"id":"aQFoPjiqsp9W"},"source":["### Get gradient"]},{"cell_type":"markdown","source":["If the `.requires_grad` of `torch.tensor` is `True`, all operations on it are ready to be tracked."],"metadata":{"id":"VEZmzYPj1zT_"}},{"cell_type":"code","metadata":{"id":"oXurRLYvsp9W"},"source":["x = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True)\n","y = torch.ones_like(x, requires_grad=True)\n","\n","print(x)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`grad_fn` references the operation creating the tensor."],"metadata":{"id":"ZbG5zT2g3uCk"}},{"cell_type":"code","metadata":{"id":"UPVaCg9Csp9W"},"source":["z = x + y\n","print(z)\n","print(z.grad_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hG5bWmCXsp9W"},"source":["z = z ** 2\n","print(z)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KeFRqU-sp9X"},"source":["o = z.mean()\n","print(o)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.retain_grad()\n","y.retain_grad()\n","z.retain_grad()\n","\n","o.backward()"],"metadata":{"id":"shGdXuGM4fO0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Slightly abuse notation by denoting the matrices, vectors as equal.\n","\n","$\\nabla_{z}o = \\frac{\\partial o}{\\partial z} = \\frac{\\partial}{\\partial z}\\frac{1}{6}\\sum_{i,j} z_{ij}=\\frac{1}{6}$\\\n","$\\nabla_{x}o = \\frac{\\partial o}{\\partial x} = \\frac{\\partial o}{\\partial z}\\cdot\\frac{\\partial z}{\\partial x} = \\frac{1}{6}\\cdot\\frac{\\partial}{\\partial x}(x+y)^2 = \\frac{1}{3}(x+y) = \\frac{x_{ij}+y_{ij}}{3}$\\\n","$\\nabla_{y}o = \\frac{\\partial o}{\\partial y} = \\frac{\\partial o}{\\partial z}\\cdot\\frac{\\partial z}{\\partial y} = \\frac{1}{6}\\cdot\\frac{\\partial}{\\partial y}(x+y)^2 = \\frac{x_{ij}+y_{ij}}{3}$"],"metadata":{"id":"zF9BBNZR6CME"}},{"cell_type":"code","source":["print(\"gradient w.r.t. z = \", z.grad)\n","print(\"gradient w.r.t. x = \", x.grad)\n","print(\"gradient w.r.t. y = \", y.grad)"],"metadata":{"id":"Z1eQP8aq4fD5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZTnY5Phpsp9Y"},"source":["### With ```torch.no_grad()```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4Kj0c60Hsp9Y"},"source":["When you evaluate your model, no need to compute gradient. To prevent tracking and back propagation, you can also wrap the code block in with `torch.no_grad()`"]},{"cell_type":"code","metadata":{"id":"NqOULboAsp9Y"},"source":["with torch.no_grad():\n","    x = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True)\n","    y = torch.ones_like(x, requires_grad=True)\n","    y = x + y\n","    z = y ** 2\n","    out = z.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxw9FVOBsp9Z"},"source":["out"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out.requires_grad"],"metadata":{"id":"BGnmUpe69ev2"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aTeg9iG_sp9Z"},"source":["out.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nje-BGjesp9Z"},"source":["## `nn.module` Neural Network Modules\n","\n","`torch.nn.module` is the base class for all neural network modules in PyTorch.\\\n","Simply saying, `nn.module` class represents a parametric function.\\\n","Do you remember the `class inheritance` that we learned in the previous lab session (Python Basics)? A child class created from a parent class can get the parent's attribute and call its method.\\\n","Due to `nn.module`, you can use many useful methods without defining.\n"]},{"cell_type":"markdown","metadata":{"id":"gXR5jfpqsp9Z"},"source":["### Using pre-defined modules"]},{"cell_type":"code","metadata":{"id":"ypn-7P5usp9a"},"source":["import torch.nn as nn\n","\n","X = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True)\n","print (X.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ad-ZLk5sp9a"},"source":["# Y = XW^T + b; input dim 3, output dim 1\n","linear_fn = nn.Linear(3, 1)\n","linear_fn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can use `nn.module` object as it is a function."],"metadata":{"id":"9OpGFIal6MZd"}},{"cell_type":"code","metadata":{"id":"smbL2japsp9a"},"source":["Y = linear_fn(X)\n","print(Y)\n","print(Y.shape) # batch_size * output_dim\n","\n","Y = Y.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kSlHALX1sp9b"},"source":["Y.backward()\n","print(X.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jvztL-YIsp9b"},"source":["You can use other types of `nn.Module` in PyTorch\n","- nn.linear\n","- nn.Conv2d\n","- nn.RNN\n","- nn.LSTM\n","- nn.GRU\n","- nn.Transformer"]},{"cell_type":"markdown","metadata":{"id":"Zlf9096Hsp9b"},"source":["### Design a neural network"]},{"cell_type":"code","metadata":{"id":"UJgtdOYrsp9c"},"source":["class MLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(MLP, self).__init__()\n","        self.linear_1 = nn.Linear(input_dim, hidden_dim)\n","        self.linear_2 = nn.Linear(hidden_dim, output_dim)\n","        self.relu = nn.ReLU()\n","    def forward(self, x):\n","        x = self.linear_1(x)\n","        x = self.relu(x) # Activation function\n","        x = self.linear_2(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tr-1kleXsp9c"},"source":["**Activation function**\n","- nn.ReLU, nn.Sigmoid, nn.Tanh, ...\n","- They make non-linearity for deep neural networks $ Y = \\sigma_1(W_1 * \\sigma_2(W_2 * \\cdots \\sigma_N(W_N * X)\\cdots))$.\n","- Deep neural networks can approximate complex functions. Without activation functions, a multi linear layer network is just a linear model $ Y = W_1 * W_2 * \\cdots * W_N * X = W_{N+1} X$.\n","\n","![picture](https://www.baeldung.com/wp-content/uploads/sites/4/2022/04/activations.png)"]},{"cell_type":"markdown","source":["### The parameters of the network"],"metadata":{"id":"k_lIEG2SEAmF"}},{"cell_type":"code","source":["model = MLP(3, 4, 1)"],"metadata":{"id":"mldURpbZEJVT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for p in model.parameters():\n","    print(p)"],"metadata":{"id":"eE8S-Gm7EF6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.state_dict()"],"metadata":{"id":"gUfU77d0Euip"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise (Train your model)\n","\n","![picture](https://www.baeldung.com/wp-content/uploads/sites/4/2022/04/artificial_neuron-1024x539.png)\n","\n","We will now implement a linear regression model. \\\n","We haven't learn about regression yet (we will learn about it in a later lab session), but don't worry, you will be able to solve this exercise even without understanding the concepts of regression.\n","\n","In this exercise, we will train a linear model approximating the following function:\n","\\begin{align*}\n","y=x^3-6x^2+11x-6\n","\\end{align*}\n","However, the linear model is:\n","\\begin{align*}\n","\\hat{y}=w_1*x_1 + w_2*x_2 + w_3*x_3 + b,\n","\\end{align*}\n","where $W=[w_1 w_2 w_3]$ is weights, and $b$ is the bias.\n","\n","How are you going to train the model?\n"],"metadata":{"id":"Zv2DHww1FksP"}},{"cell_type":"markdown","metadata":{"id":"Dr5dJKxOsp9e"},"source":["**Load packages**"]},{"cell_type":"code","metadata":{"id":"AEssd29Vsp9e"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","\n","seed = 7\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Generate data**\n","\n","The follow is the function we want to approximate."],"metadata":{"id":"KBvRFLk-vsCW"}},{"cell_type":"code","source":["x = np.linspace(0.5, 3.5, 100)\n","y = x**3 - 6*x**2 + 11*x - 6\n","\n","plt.plot(x, y, color='blue', label='Ground Truth')\n","plt.grid()\n","plt.legend()\n","plt.show()"],"metadata":{"id":"v9lfu55pQ6bJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Linear model**"],"metadata":{"id":"pM0NYUJHxj61"}},{"cell_type":"code","source":["class LinearModel(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(LinearModel, self).__init__()\n","        self.linear = nn.Linear(input_dim, output_dim)\n","\n","    def forward(self, x):\n","        out = self.linear(x)\n","        return out"],"metadata":{"id":"Ycq3TLEIS9D8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Exercise 1**. construct an appropriate model for this exercise."],"metadata":{"id":"jTknjm6IsGCr"}},{"cell_type":"code","source":["model = # Enter your code"],"metadata":{"id":"ypHiJJUtT6-x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Exercise 2**. How should you create input data for your linear model? your linear model"],"metadata":{"id":"09hi3TjHsBYT"}},{"cell_type":"code","source":["x_for_model = # Enter your code\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, x, y):\n","        self.x_data = torch.FloatTensor(x)\n","        self.y_data = torch.FloatTensor(y.reshape(-1, 1))\n","\n","    def __len__(self):\n","        return len(self.x_data)\n","\n","    def __getitem__(self, idx):\n","        return self.x_data[idx], self.y_data[idx]\n","\n","train_data = CustomDataset(x_for_model, y)\n","train_loader = DataLoader(train_data, batch_size=10, shuffle=True) # Total data size 100, mini batch size 10"],"metadata":{"id":"VsJwPnEHX_Dp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Exercise 3**. complete the `plot_model` to plot your current model."],"metadata":{"id":"C_JtDr45sIvH"}},{"cell_type":"code","source":["def plot_model(model):\n","    with torch.no_grad():\n","        #################################\n","\n","        # Enter your code\n","\n","        #################################\n","\n","    plt.plot(x, y_hat, color='red', label='your model')\n","    plt.plot(x, y, color='blue', label='Ground Truth')\n","    plt.grid()\n","    plt.legend()\n","    plt.show()\n","\n","print(\"your initial model: \", model.state_dict())\n","plot_model(model)"],"metadata":{"id":"e-PA43cshjdW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Define Loss and optimizer**\n","\n","We set mean square error loss as the loss function\\\n","and use Adam Optimization as the optimizer for the model.\n","\n","In simple term, Adam Optimization is a variant of SGD (sotchastic gradient descent). We won't handle the details for the optimizer in this class.\\\n","Those who are curious about this can refer to [here](https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0).\n","\n","![picture](https://miro.medium.com/v2/resize:fit:640/1*XVFmo9NxLnwDr3SxzKy-rA.gif)\n"],"metadata":{"id":"_EZXUZiYyZ9o"}},{"cell_type":"code","source":["loss_function = nn.MSELoss()\n","\n","optimizer = optim.Adam(params=model.parameters(), lr=0.01)"],"metadata":{"id":"Y3rbwlVBr_6G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train your model**"],"metadata":{"id":"xNZZAmVm0zm7"}},{"cell_type":"code","source":["train_epochs = 25000\n","loss_list = []\n","epoch_list = []\n","\n","for epoch in range(train_epochs):\n","    average_loss = []\n","    steps = 0\n","\n","    for i, (x_train, y_train) in enumerate(train_loader):\n","        pred = model(x_train)\n","        loss = loss_function(pred, y_train)\n","        average_loss.append(loss.detach().numpy())\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if (epoch+1) % 500 == 0:\n","        print('Epoch: ',  epoch+1, ' / Loss: ', np.mean(average_loss), ' /model parameter: ', model.state_dict())\n","        loss_list.append(np.mean(average_loss))\n","        epoch_list.append(epoch+1)\n","\n","plt.plot(epoch_list, loss_list, label='train_loss')\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.show()"],"metadata":{"id":"u_pbm5m3VNxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Trained model: \", model.state_dict())\n","plot_model(model)"],"metadata":{"id":"vy-X3h2afHDh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## References\n","https://pytorch.org/docs/stable/index.html\n","\n","https://www.simplilearn.com/keras-vs-tensorflow-vs-pytorch-article\n","\n","https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2023/\n","\n","https://github.com/wkentaro/pytorch-for-numpy-users\n","\n","https://www.baeldung.com/cs/convolutional-vs-regular-nn\n","\n","https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0"],"metadata":{"id":"ZSDn8-3yIiVS"}}]}